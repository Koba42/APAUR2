{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RETO 2 APAU - GRUPO 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importación Librerias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import csv\n",
    "import re\n",
    "from string import punctuation\n",
    "from nltk.corpus import stopwords\n",
    "import unicodedata\n",
    "from collections import Counter\n",
    "import emoji\n",
    "\n",
    "# Libreria para vectorizar texto\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Librerias para el clustering\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import silhouette_samples, silhouette_score\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "\n",
    "# Librerias para reduccion de dimensionalidad\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "from sklearn.cluster import DBSCAN\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Funciones Utilizadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convertimos a minusculas, sacamos enlaces etc...\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()  # Lowercase text\n",
    "    text = re.sub(r\"https?://\\S+\", \"\", text)\n",
    "    text = re.sub(r\"\\b[0-9]+\\b\\s*\", \"\", text)\n",
    "    text = re.sub(r\"@[\\w]+\", \"\", text)  # Eliminar menciones de usuario\n",
    "    text = re.sub(r\"[^a-zA-Záéíóúüñ\\s]\", \"\", text)  # Eliminar caracteres no alfabéticos\n",
    "    text = re.sub(f\"[{re.escape(punctuation)}]\", \"\", text)  # Remove punctuation\n",
    "    text = \" \".join(text.split())  # Remove extra spaces, tabs, and new lines\n",
    "    return text\n",
    "\n",
    "# Función para eliminar stopwords\n",
    "def remove_stopwords(text):\n",
    "    # Convertir texto a minúsculas y eliminar acentos\n",
    "    text = unicodedata.normalize('NFD', text).encode('ascii', 'ignore').decode('utf-8')\n",
    "    return \" \".join([word for word in str(text).lower().split() if word not in STOPWORDS])\n",
    "\n",
    "def convert_emoji_to_text(emoji_text):\n",
    "    text_with_aliases = emoji.demojize(emoji_text, delimiters=(\":\", \":\"))\n",
    "    return text_with_aliases\n",
    "\n",
    "\n",
    "\n",
    "def Plot2D (samples_2D, title, axes = {'x': 'x', 'y': 'y'}):\n",
    "\n",
    "  \"\"\"\n",
    "  samples_2D (numpy.ndarray): array tipo embedding cuyo\n",
    "        shape es (num_muestras, 2) ; 2 por las dos coordenadas\n",
    "\n",
    "  title(string): titulo de la figura que se utiliza para el plot\n",
    "  \"\"\"\n",
    "\n",
    "  df_samples_2D = pd.DataFrame(data=samples_2D, columns=[axes['x'], axes['y']])\n",
    "\n",
    "  sns.set(font_scale=3)\n",
    "  sns.set(rc={'figure.figsize':(10,10)})\n",
    "  sns.relplot(data=df_samples_2D,\n",
    "              x=axes['x'],\n",
    "              y=axes['y'],\n",
    "              height=10, legend=\"full\", palette=\"bright\")\n",
    "\n",
    "  set_size_letters(title,\n",
    "                   axes['x'],\n",
    "                   axes['y'],\n",
    "                   active_legend = False)\n",
    "  plt.axis('equal')\n",
    "\n",
    "def set_size_letters(title, x_name, y_name, title_size = 20, x_size = 18, y_size = 18, active_legend = True, legend_size = 14):\n",
    "\n",
    "  \"\"\"\n",
    "  Parameters:\n",
    "    title (string): titulo del plot a representar\n",
    "\n",
    "    x_name (string): nombre del eje x\n",
    "\n",
    "    y_name (string): nombre del eje y\n",
    "\n",
    "    active_legend (bool): indica si mostramos la leyenda o no. Por defecto True\n",
    "\n",
    "    x_size, y_size, legend_size: tamaño de fuente de eje x, y, leyenda\n",
    "\n",
    "  \"\"\"\n",
    "\n",
    "  plt.title(title, fontsize=title_size)\n",
    "  plt.xlabel(x_name, fontsize=x_size)\n",
    "  plt.ylabel(y_name, fontsize=y_size)\n",
    "  if (active_legend == True):\n",
    "    plt.legend(fontsize=legend_size)\n",
    "\n",
    "def Plot3D_WithLabels (samples_3D, labels, title, axes = {'x': 'x', 'y': 'y', 'z': 'z'}, centroids_3D=None):\n",
    "\n",
    "  \"\"\"\n",
    "  samples_3D (numpy.ndarray): array tipo embedding cuyo\n",
    "        shape es (num_muestras, 3) ; 3 por las tres coordenadas\n",
    "\n",
    "  labels (array): etiqueta perteneciente a cada muestra.\n",
    "\n",
    "  title(string): titulo de la figura que se utiliza para el plot\n",
    "  \"\"\"\n",
    "\n",
    "  # First we create the dataframe\n",
    "  df_samples_3D_labeled = pd.DataFrame(data=samples_3D, columns=[axes['x'], axes['y'], axes['z']])\n",
    "\n",
    "  # Then we add the labels column\n",
    "  df_samples_3D_labeled['label'] = labels.tolist()\n",
    "  df_samples_3D_labeled['label'] = df_samples_3D_labeled[\"label\"].astype(str)\n",
    "\n",
    "  if centroids_3D is not None:\n",
    "    # Same applies for centroids when these are provided as an argument\n",
    "    labels_centroid_aux = np.arange(len(centroids_3D), dtype=int)\n",
    "    df_centroids_3D_labeled = pd.DataFrame(data=centroids_3D, columns=[axes['x'], axes['y'], axes['z']])\n",
    "    df_centroids_3D_labeled['label'] = labels_centroid_aux.tolist()\n",
    "\n",
    "    # We create an additional column with the dot size used for each type of sample\n",
    "    size_no_centroid = np.ones(len(samples_3D)) * 10 # for regular samples\n",
    "    size_centroid = np.ones(len(centroids_3D)) * 50 # for centroids\n",
    "    size_col = np.append(size_no_centroid, size_centroid) # new col to be added to the dataframe\n",
    "\n",
    "    # We also create another additional column with the labels for each type of sample\n",
    "    no_es_centroide_aux = [' '] * len(samples_3D) # empty label for regular samples\n",
    "    es_centroide_aux = []\n",
    "    for i in range(len(centroids_3D)):\n",
    "      es_centroide_aux.append('C%d' % i) # Ci label for centroid i\n",
    "    centroid_col = no_es_centroide_aux + es_centroide_aux # new col to be added to the dataframe\n",
    "\n",
    "    # Next we concatenate both dataframes: first, regular samples, then, centroids\n",
    "    df_samples_and_centroids = pd.concat([df_samples_3D_labeled, df_centroids_3D_labeled], ignore_index=True)\n",
    "      \n",
    "\n",
    "    # We add the new column with the labels distinguishing regular samples from centroids\n",
    "    df_samples_and_centroids['centroid'] = centroid_col\n",
    "\n",
    "    # New column is re-casted as a string column\n",
    "    df_samples_and_centroids['centroid'] = df_samples_and_centroids['centroid'].astype(str)\n",
    "\n",
    "    # We add the new column with the corresponding size for both regular samples and centroids\n",
    "    df_samples_and_centroids['size'] = size_col\n",
    "\n",
    "    # We ensure that the 'label' column is numeric since we will sort the dataframe upon this one\n",
    "    df_samples_and_centroids['label'] = pd.to_numeric(df_samples_and_centroids['label'])\n",
    "\n",
    "    # We finally sort the dataframe by the 'label' column in ascending order\n",
    "    df_samples_and_centroids_sorted = df_samples_and_centroids.sort_values(by=['label'], ascending=True)\n",
    "\n",
    "    # And plot both the samples and their corresponding centroids\n",
    "    fig = px.scatter_3d(df_samples_and_centroids_sorted, x=axes['x'], y=axes['y'], z=axes['z'], text='centroid', size='size', color='label')\n",
    "  else:\n",
    "    # We ensure that the 'label' column is numeric since we will sort the dataframe upon this one\n",
    "    df_samples_3D_labeled['label'] = pd.to_numeric(df_samples_3D_labeled['label'])\n",
    "\n",
    "    # We finally sort the dataframe by the 'label' column in ascending order\n",
    "    df_samples_and_centroids_sorted = df_samples_3D_labeled.sort_values(by=['label'], ascending=True)\n",
    "\n",
    "    fig = px.scatter_3d(df_samples_and_centroids_sorted, x=axes['x'], y=axes['y'], z=axes['z'], color='label', size=np.ones(len(samples_3D))) #, color_continuous_scale='delta')\n",
    "\n",
    "  fig.update_traces(textposition='top center')\n",
    "  fig.update_layout(scene_aspectmode='data')\n",
    "  fig.update_layout(uniformtext_minsize=60)\n",
    "\n",
    "  fig.update_layout(title_font_size=20,\n",
    "                    title={\n",
    "                    'text': title,\n",
    "                    'y': 0.9,\n",
    "                    'x': 0.05,\n",
    "                    'xanchor': 'left',\n",
    "                    'yanchor': 'top'})\n",
    "  # tight layout\n",
    "  fig.update_layout(autosize = True, margin = dict(l=50, r=0, b=10, t=30))\n",
    "  fig.show()\n",
    "\n",
    "\n",
    "# Función para obtener los términos más representativos por clúster\n",
    "def get_top_terms_by_cluster(clustered_tweets, top_n=5):\n",
    "    cluster_topics = {}\n",
    "    for cluster in clustered_tweets['label'].unique():\n",
    "        # Filtrar los tweets del clúster actual\n",
    "        cluster_tweets = clustered_tweets[clustered_tweets['label'] == cluster]['tweet']\n",
    "\n",
    "        # Tokenizar palabras\n",
    "        all_words = \" \".join(cluster_tweets).split()\n",
    "\n",
    "        # Contar frecuencia de palabras\n",
    "        word_counts = Counter(all_words)\n",
    "\n",
    "        # Obtener las 'top_n' palabras más frecuentes\n",
    "        cluster_topics[cluster] = word_counts.most_common(top_n)\n",
    "    return cluster_topics\n",
    "\n",
    "def plot_2D_centroid_labels(df_centroids, ax):\n",
    "    for index, row in df_centroids.iterrows():\n",
    "        ax.text(row[0], row[1], 'C'+str(row[2].astype(int)), \n",
    "                fontsize=20, color='black', weight='semibold')\n",
    "        \n",
    "\n",
    "# Función para obtener los términos más representativos por clúster\n",
    "def get_top_terms_by_cluster(clustered_tweets, top_n=5):\n",
    "    cluster_topics = {}\n",
    "    for cluster in clustered_tweets['label'].unique():\n",
    "        # Filtrar los tweets del clúster actual\n",
    "        cluster_tweets = clustered_tweets[clustered_tweets['label'] == cluster]['tweet']\n",
    "\n",
    "        # Tokenizar palabras\n",
    "        all_words = \" \".join(cluster_tweets).split()\n",
    "\n",
    "        # Contar frecuencia de palabras\n",
    "        word_counts = Counter(all_words)\n",
    "\n",
    "        # Obtener las 'top_n' palabras más frecuentes\n",
    "        cluster_topics[cluster] = word_counts.most_common(top_n)\n",
    "    return cluster_topics\n",
    "\n",
    "\n",
    "def Plot2D_WithLabels (samples_2D, labels, title, axes = {'x': 'x', 'y': 'y'}, palette=\"bright\", centroids_2D=None):\n",
    "\n",
    "  \"\"\"\n",
    "  samples_2D (numpy.ndarray): array tipo embedding cuyo\n",
    "        shape es (num_muestras, 2) ; 2 por las dos coordenadas\n",
    "\n",
    "  labels (array): etiqueta perteneciente a cada muestra.\n",
    "\n",
    "  title(string): titulo de la figura que se utiliza para el plot\n",
    "  \"\"\"\n",
    "\n",
    "  # First we create the dataframe\n",
    "  df_samples_2D_labeled = pd.DataFrame(data=samples_2D, columns=[axes['x'], axes['y']])\n",
    "\n",
    "  # Then we add the labels column\n",
    "  df_samples_2D_labeled['label'] = labels.tolist()\n",
    "\n",
    "  if centroids_2D is not None:\n",
    "    labels_centroid_aux = np.arange(len(centroids_2D), dtype=int)\n",
    "    df_centroids_2D_labeled = pd.DataFrame(data=centroids_2D, columns=[axes['x'], axes['y']])\n",
    "    df_centroids_2D_labeled['label'] = labels_centroid_aux.tolist()\n",
    "\n",
    "  sns.set(font_scale=3)\n",
    "  sns.set(rc={'figure.figsize':(10,10)})\n",
    "  sns.relplot(data=df_samples_2D_labeled,\n",
    "              x=axes['x'],\n",
    "              y=axes['y'],\n",
    "              hue=\"label\", height=10, legend=\"full\", palette=palette)\n",
    "\n",
    "  if centroids_2D is not None:\n",
    "    # First we plot the centroids\n",
    "    sns.scatterplot(data=df_centroids_2D_labeled,\n",
    "              x=axes['x'],\n",
    "              y=axes['y'],\n",
    "              hue=\"label\",\n",
    "              legend=False, palette=palette, s=100)\n",
    "\n",
    "    # Then we plot their labels\n",
    "    plot_2D_centroid_labels(df_centroids_2D_labeled, plt.gca())\n",
    "\n",
    "  set_size_letters(title,\n",
    "                   axes['x'],\n",
    "                   axes['y'],\n",
    "                   active_legend = False)\n",
    "  plt.axis('equal')\n",
    "    \n",
    "    \n",
    "\n",
    "def ApplyDBScanToData (samples, epsilon, min_samples = 30):\n",
    "\n",
    "  \"\"\"\n",
    "  Parameters:\n",
    "\n",
    "    samples (numpy.ndarray): array tipo embedding cuyo\n",
    "        shape es (n_ejemplo,n_muestras_por_ejemplo)\n",
    "\n",
    "    epsilon: int con el valor de epsilon (distancia mínima entre elementos\n",
    "        para formar un cluster)\n",
    "\n",
    "    min_samples (int): numero minimo de muestras para generar un cluster\n",
    "\n",
    "    include_noise (bool): permite seleccionar si se quiere incluir el cluster\n",
    "        de ruido en el cálculo de la silueta o no. Por defecto no se incluye.\n",
    "        Se debe incluir cuando únicamente se detecta un cluster, para poder\n",
    "        realizar la evaluación.\n",
    "\n",
    "  Return:\n",
    "\n",
    "    labels: lista con el cluster al que pertenece cada ejemplo de data. Cluster\n",
    "        -1 significa ruido.\n",
    "  \"\"\"\n",
    "\n",
    "\n",
    "  db = DBSCAN(eps=epsilon, min_samples=min_samples).fit(samples)\n",
    "\n",
    "  core_samples_mask = np.zeros_like(db.labels_, dtype=bool)\n",
    "  core_samples_mask[db.core_sample_indices_] = True\n",
    "  labels = db.labels_\n",
    "\n",
    "  # Number of clusters in labels, ignoring noise if present.\n",
    "  n_clusters_ = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "  n_noise_ = list(labels).count(-1)\n",
    "\n",
    "  print(f\"\\nTest for epsilon = {epsilon}\")\n",
    "  print('Estimated number of clusters: %d' % n_clusters_)\n",
    "  print('Estimated number of noise points: %d' % n_noise_)\n",
    "  print(\"Silhouette Coefficient: %0.3f\" % metrics.silhouette_score(samples, labels))\n",
    "\n",
    "\n",
    "  return labels\n",
    "\n",
    "\n",
    "def PlotDistancesToKnearestNeighbor(data_vector, K):\n",
    "\n",
    "  \"\"\"\n",
    "  Parameters:\n",
    "    data_vector (numpy.ndarray): array tipo embedding cuyo\n",
    "        shape es (n_ejemplo,n_muestras_por_ejemplo)\n",
    "\n",
    "    k: posición del k-esimo vecino más cercano\n",
    "\n",
    "  \"\"\"\n",
    "\n",
    "  nbrs = NearestNeighbors(n_neighbors=K).fit(data_vector)\n",
    "  distances, indices = nbrs.kneighbors(data_vector)\n",
    "  distances = np.sort(distances, axis=0)\n",
    "  distances = distances[:,K-1]\n",
    "  plt.figure(figsize=(10,8))\n",
    "  set_size_letters(f\"Distancias al K-vecino más cercano (K={K})\",\n",
    "                   f\"Points sorted according to distance of the {K}-th nearest neighbor\",\n",
    "                   f\"{K}-th nearest neighbor distance\")\n",
    "  plt.plot(distances)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Archivo APAUtweets.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Carga de Datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_tweet = pd.read_csv(\"APAUtweets.txt\", header=None, delimiter = \"\\t\", encoding = 'utf8', quoting=csv.QUOTE_NONE)\n",
    "dataset_tweet.set_index(0, inplace = True)\n",
    "dataset_tweet.rename(columns={1: \"text\"}, inplace = True)\n",
    "dataset_tweet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Limpiar y procesar dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_tweet[\"text_lower\"] = dataset_tweet[\"text\"].map(preprocess_text) # Se aplica la función preprocess_text a la columna text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Borramos palabras tipicas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lista de stopwords en español\n",
    "STOPWORDS = set(stopwords.words('spanish'))\n",
    "# Aplicar la función a la columna del dataset\n",
    "dataset_tweet[\"text_stopW\"] = dataset_tweet[\"text_lower\"].apply(remove_stopwords)\n",
    "\n",
    "dataset_tweet\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 Contamos palabras que más se repiten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Contamos las palabras más comunes utilizadas en los tweets, nos sirve para tener una idea de los temas más comunes en los tweets y para procesar nuevas palabras que no estén en el diccionario de stopwords\n",
    "cnt = Counter()\n",
    "for text in dataset_tweet[\"text_stopW\"].values:\n",
    "    for word in text.split():\n",
    "        cnt[word] += 1\n",
    "        \n",
    "cnt.most_common(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cargar stopwords en español y agregar palabras personalizadas\n",
    "STOPWORDS = set(stopwords.words('spanish'))\n",
    "STOPWORDS.update([\"user\",\"mas\", \"hoy\", \"si\", \"mejor\", \"dia\", \"mundo\"])  # Agrega palabras específicas\n",
    "\n",
    "dataset_tweet[\"text_stopW\"] = dataset_tweet[\"text_lower\"].apply(remove_stopwords)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Contar palabras más frecuentes\n",
    "cnt = Counter()\n",
    "for text in dataset_tweet[\"text_stopW\"].values:\n",
    "    for word in text.split():\n",
    "        cnt[word] += 1\n",
    "\n",
    "# Mostrar las 15 palabras más comunes\n",
    "cnt.most_common(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5 Convertimos los emojis a texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Passing both functions to 'text_rare'\n",
    "dataset_tweet['text_rare'] = dataset_tweet['text_stopW'].apply(convert_emoji_to_text)\n",
    "\n",
    "dataset_tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Se eliminan los emojis de los tweets\n",
    "\n",
    "\n",
    "#def erase_emoji(emoji_text):\n",
    "#    texto_sin_emojis = emoji.replace_emoji(emoji_text, replace='')\n",
    "#    return texto_sin_emojis\n",
    "\n",
    "\n",
    "\n",
    "# Passing both functions to 'text_rare'\n",
    "#dataset_tweet['text_rare'] = dataset_tweet['text_stopW'].apply(erase_emoji)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.6 World Embedding con TfidVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lista_descriptiva = dataset_tweet[\"text_rare\"]\n",
    "vectorizer = TfidfVectorizer(use_idf=False, min_df=.0005)\n",
    "matrix = vectorizer.fit_transform(lista_descriptiva)\n",
    "matrix_text = pd.DataFrame(matrix.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "samples = matrix_text\n",
    "samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.7 KMEANS sin PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculo kmeans del texto sin aplicar PCA para demostrar que si no aplicamos PCA para reducir dimensiones, el silhouette score es muy bajo debido a la maldición de la dimensionalidad.\n",
    "for i in range(2,20):\n",
    "    km = KMeans(n_clusters=i, init='random', max_iter=200, random_state=0).fit(samples)\n",
    "    sample_km = km.labels_\n",
    "    print(\"cantidad de cluster:\", i)\n",
    "    print(\"Silhouette Coefficient: %0.3f\" % metrics.silhouette_score(samples, sample_km))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.8 Reducción por PCA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Aplicamos PCA a sam para reducir dimensiones a 2D\n",
    "pca2D = PCA(2)\n",
    "pca2D.fit(samples)\n",
    "#Transform the data\n",
    "sample_PCA_2D = pca2D.transform(samples)\n",
    "\n",
    "df_samples_PCA_2D = pd.DataFrame(data=sample_PCA_2D, columns=[\"pca0\", \"pca1\"])\n",
    "\n",
    "title = 'Original data after 2D PCA transform'\n",
    "axes_PCA_2D = {'x': 'pca0', 'y': 'pca1'}\n",
    "Plot2D (sample_PCA_2D, title, axes_PCA_2D)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.8.1 Modelación de Vecinos cercanos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PlotDistancesToKnearestNeighbor(sample_PCA_2D, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.8.2 Aplicamos DBSCAN a los datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.8.2.1 Buscamos el epsilon optimo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Buscamos el Epsilon óptimo para el DBSCAN y aplicamos el algoritmo\n",
    "min_samples = 10\n",
    "epsilon_values = [0.007, 0.0075, 0.0076, 0.0077, 0.0078, 0.0079, 0.008, 0.009, 0.021, 0.023, 0.024, 0.025,0.026]\n",
    "# con user: epsilon_values = [0.020, 0.022, 0.024, 0.026, 0.028, 0.030, 0.032, 0.034, 0.036, 0.038, 0.040]\n",
    "aux_labels = {}\n",
    "for epsilon in epsilon_values:\n",
    "  aux_labels[epsilon] = ApplyDBScanToData (sample_PCA_2D, epsilon, min_samples=min_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# con user: epsilon = 0.034\n",
    "epsilon = 0.0078 #602\n",
    "labels_PCA_2D = aux_labels[epsilon]\n",
    "\n",
    "# Crear un DataFrame con los tweets y las etiquetas de clúster\n",
    "tweets_with_labels = pd.DataFrame({\n",
    "    'tweet': lista_descriptiva,  # Cambia esto por tu columna de tweets procesados\n",
    "    'label': labels_PCA_2D\n",
    "})\n",
    "\n",
    "# Filtrar para ignorar los puntos de ruido\n",
    "clustered_tweets = tweets_with_labels[tweets_with_labels['label'] != -1]\n",
    "clustered_tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.8.2.2 Representación del Resultado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title = f'DBScan(eps={epsilon}, MinPts={min_samples}) over 2D PCA transformed data'\n",
    "Plot2D_WithLabels (sample_PCA_2D, labels_PCA_2D, title, axes_PCA_2D)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.8.2.3 Obtener terminos más representativos de c/Cluster para verificar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtener los términos más representativos\n",
    "top_terms_per_cluster = get_top_terms_by_cluster(clustered_tweets)\n",
    "top_terms_per_cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.8.3 Aplicamos K-Means a los datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.8.3.1 Buscamos la cantidad de clusters optima y aplicamos KMEANS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculo kmeans del texto con aplicar PCA\n",
    "clusters =[2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20]\n",
    "silhouette_avg = []\n",
    "\n",
    "\n",
    "for i in clusters:\n",
    "    km = KMeans(n_clusters=i, init='random', max_iter=200, random_state=0).fit(sample_PCA_2D)\n",
    "    sample_km_pca_2d = km.labels_\n",
    "    \n",
    "    score = silhouette_score(sample_PCA_2D, km.labels_)\n",
    "    silhouette_avg.append(score)\n",
    "    \n",
    "    print(\"cantidad de cluster:\", i)\n",
    "    print(\"Silhouette Coefficient: %0.3f\" % metrics.silhouette_score(sample_PCA_2D, sample_km_pca_2d))\n",
    "\n",
    "#3 clusters - 0.722"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.8.3.2 Grafico KMEANS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_clusters = clusters[pd.Series(silhouette_avg).idxmax()]\n",
    "kmeans = KMeans(n_clusters=n_clusters)\n",
    "label = kmeans.fit_predict(sample_PCA_2D)\n",
    "\n",
    "\n",
    "# Crear un DataFrame con los tweets y las etiquetas de clúster\n",
    "tweets_with_labels = pd.DataFrame({\n",
    "    'tweet': lista_descriptiva,  # Cambia esto por tu columna de tweets procesados\n",
    "    'label': label\n",
    "})\n",
    "\n",
    "# Filtrar para ignorar los puntos de ruido\n",
    "clustered_tweets = tweets_with_labels[tweets_with_labels['label'] != -1]\n",
    "clustered_tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "axes_PCA = {'x': 'pca0', 'y': 'pca1'}\n",
    "title = f'K-means(K={n_clusters}) over 2D PCA transformed data'\n",
    "plot = Plot2D_WithLabels(sample_PCA_2D, label, title, axes_PCA , centroids_2D=kmeans.cluster_centers_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.8.3.3 Obtener terminos más representativos de c/Cluster para verificar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtener los términos más representativos\n",
    "top_terms_per_cluster = get_top_terms_by_cluster(clustered_tweets)\n",
    "top_terms_per_cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.9 Reducción por t-SNE 2D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.9.1 Creación del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne2D = TSNE(n_components=2)\n",
    "samples_TSNE_2D = tsne2D.fit_transform(samples)\n",
    "df_samples_TSNE_2D = pd.DataFrame(data=samples_TSNE_2D, columns=[\"tsne0\", \"tsne1\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.9.2 Representación Visual del Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title = 'Original data after 2D t-SNE transform'\n",
    "axes_TSNE_2D = {'x': 'tsne0', 'y': 'tsne1'}\n",
    "Plot2D (samples_TSNE_2D, title, axes_TSNE_2D)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.9.3 Aplicamos K-Means t-SNE 2D a los datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.9.3.1 Buscamos la cantidad de clusters optima y aplicamos KMEANS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculo kmeans del texto con aplicar PCA\n",
    "clusters =[2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20]\n",
    "silhouette_avg = []\n",
    "\n",
    "\n",
    "for i in clusters:\n",
    "    km = KMeans(n_clusters=i, init='random', max_iter=200, random_state=0).fit(samples_TSNE_2D)\n",
    "    sample_km_pca_2d = km.labels_\n",
    "    \n",
    "    score = silhouette_score(samples_TSNE_2D, km.labels_)\n",
    "    silhouette_avg.append(score)\n",
    "    \n",
    "    print(\"cantidad de cluster:\", i)\n",
    "    print(\"Silhouette Coefficient: %0.3f\" % metrics.silhouette_score(samples_TSNE_2D, sample_km_pca_2d))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.9.3.2 Graficamos KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_clusters = clusters[pd.Series(silhouette_avg).idxmax()]\n",
    "kmeans = KMeans(n_clusters=n_clusters)\n",
    "label = kmeans.fit_predict(samples_TSNE_2D)\n",
    "\n",
    "# Crear un DataFrame con los tweets y las etiquetas de clúster\n",
    "tweets_with_labels = pd.DataFrame({\n",
    "    'tweet': lista_descriptiva,  # Cambia esto por tu columna de tweets procesados\n",
    "    'label': label\n",
    "})\n",
    "\n",
    "# Filtrar para ignorar los puntos de ruido\n",
    "clustered_tweets = tweets_with_labels[tweets_with_labels['label'] != -1]\n",
    "clustered_tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "axes_tSNE = {'x': 'tsne0', 'y': 'tsne1'}\n",
    "title = f'K-means(K={n_clusters}) over 2D T-SNE transformed data'\n",
    "plot = Plot2D_WithLabels(samples_TSNE_2D, kmeans.labels_, title, axes_tSNE, centroids_2D=kmeans.cluster_centers_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.9.3.3 Obtener terminos más representativos de c/Cluster para verificar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Función para obtener los términos más representativos por clúster\n",
    "def get_top_terms_by_cluster(clustered_tweets, top_n=5):\n",
    "    cluster_topics = {}\n",
    "    for cluster in clustered_tweets['label'].unique():\n",
    "        # Filtrar los tweets del clúster actual\n",
    "        cluster_tweets = clustered_tweets[clustered_tweets['label'] == cluster]['tweet']\n",
    "\n",
    "        # Tokenizar palabras\n",
    "        all_words = \" \".join(cluster_tweets).split()\n",
    "\n",
    "        # Contar frecuencia de palabras\n",
    "        word_counts = Counter(all_words)\n",
    "\n",
    "        # Obtener las 'top_n' palabras más frecuentes\n",
    "        cluster_topics[cluster] = word_counts.most_common(top_n)\n",
    "    return cluster_topics\n",
    "\n",
    "# Obtener los términos más representativos\n",
    "top_terms_per_cluster = get_top_terms_by_cluster(clustered_tweets)\n",
    "top_terms_per_cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.9.4 Modelación KnearestNeighbor TSNE 2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PlotDistancesToKnearestNeighbor(samples_TSNE_2D, 15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.9.5 Aplicar DBScan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.9.5.1 Encontrar Epsilon óptimo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_samples = 20\n",
    "epsilon_values = [3, 3.2, 3.4, 3.6, 3.8, 3.9, 4, 4.05, 4.055,4.1, 4.3, 4.5, 4.7, 4.9, 5.0, 5.1]\n",
    "aux_labels = {}\n",
    "\n",
    "for epsilon in epsilon_values:\n",
    "  aux_labels[epsilon] = ApplyDBScanToData (samples_TSNE_2D, epsilon, min_samples=min_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#epsilon = 3.4\n",
    "epsilon = 4.9\n",
    "labels_TSNE_2D = aux_labels[epsilon]\n",
    "\n",
    "\n",
    "# Crear un DataFrame con los tweets y las etiquetas de clúster\n",
    "tweets_with_labels = pd.DataFrame({\n",
    "    'tweet': lista_descriptiva,  # Cambia esto por tu columna de tweets procesados\n",
    "    'label': labels_TSNE_2D\n",
    "})\n",
    "\n",
    "# Filtrar para ignorar los puntos de ruido\n",
    "clustered_tweets = tweets_with_labels[tweets_with_labels['label'] != -1]\n",
    "clustered_tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.9.5.2 Representación grafica del resultado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title = f'DBScan(eps={epsilon}, MinPts={min_samples}) over 2D t-SNE transformed data'\n",
    "Plot2D_WithLabels (samples_TSNE_2D, labels_TSNE_2D, title, axes_TSNE_2D)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.9.5.3 Obtener terminos más representativos de c/Cluster para verificar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Función para obtener los términos más representativos por clúster\n",
    "def get_top_terms_by_cluster(clustered_tweets, top_n=5):\n",
    "    cluster_topics = {}\n",
    "    for cluster in clustered_tweets['label'].unique():\n",
    "        # Filtrar los tweets del clúster actual\n",
    "        cluster_tweets = clustered_tweets[clustered_tweets['label'] == cluster]['tweet']\n",
    "\n",
    "        # Tokenizar palabras\n",
    "        all_words = \" \".join(cluster_tweets).split()\n",
    "\n",
    "        # Contar frecuencia de palabras\n",
    "        word_counts = Counter(all_words)\n",
    "\n",
    "        # Obtener las 'top_n' palabras más frecuentes\n",
    "        cluster_topics[cluster] = word_counts.most_common(top_n)\n",
    "    return cluster_topics\n",
    "\n",
    "# Obtener los términos más representativos\n",
    "top_terms_per_cluster = get_top_terms_by_cluster(clustered_tweets)\n",
    "top_terms_per_cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Embeding BETO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Carga de Datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_beto = np.load(\"EmoEvalEs-embeddings-BETO.npy\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Sin reducción de dimensionalidad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.1 Nearest Neighbor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PlotDistancesToKnearestNeighbor(embeddings_beto, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.2 DBScan sin Reducción de Dimensionalidad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_samples = 10\n",
    "epsilon_values = [4,4.1,4.2,4.3,4.4,4.5,4.6,4.7,4.8,5,6]\n",
    "aux_labels = {}\n",
    "for epsilon in epsilon_values:\n",
    "  aux_labels[epsilon] = ApplyDBScanToData(embeddings_beto, epsilon, min_samples=min_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.3 K-Means sin Reducción de Dimensionalidad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(2,20):\n",
    "    km = KMeans(n_clusters=i, init='random', max_iter=200, random_state=0).fit(embeddings_beto)\n",
    "    sample_km = km.labels_\n",
    "    score = metrics.silhouette_score(embeddings_beto, sample_km)\n",
    "    print(\"cantidad de cluster:\", i)\n",
    "    print(\"Silhouette Coefficient: %0.3f\" % score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Con reducción de dimensionalidad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.1 K-Means PCA 2D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.1.1 Búsqueda N° óptimo CLusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Raw Data\n",
    "pca2D = PCA(2)\n",
    " \n",
    "#Transform the data\n",
    "df_2D = pca2D.fit_transform(embeddings_beto)\n",
    "\n",
    "df_samples_PCA_2D = pd.DataFrame(data=df_2D, columns=[\"pca0\", \"pca1\"])\n",
    "\n",
    "clusters = [5,6,7,8,9,10,15]\n",
    "silhouette_avg = []\n",
    "\n",
    "for i in clusters:\n",
    "    #Initialize the class object\n",
    "    kmeans = KMeans(n_clusters=i)\n",
    "     \n",
    "    #predict the labels of clusters.\n",
    "    label = kmeans.fit_predict(df_samples_PCA_2D)\n",
    "     \n",
    "    #Getting unique labels\n",
    "    u_labels = np.unique(label)\n",
    "     \n",
    "    #plotting the results:\n",
    "    for i in u_labels:\n",
    "        plt.scatter(df_2D[label == i , 0] , df_2D[label == i , 1] , label = i)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    score = silhouette_score(df_samples_PCA_2D, kmeans.labels_)\n",
    "    silhouette_avg.append(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.1.2 Modelacion N° clusters - Score Silhouette"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(clusters,silhouette_avg)\n",
    "plt.xlabel(\"Values of K\") \n",
    "plt.ylabel(\"Silhouette score\") \n",
    "plt.title(\"Silhouette analysis For Optimal k\")\n",
    "plt.show()\n",
    "print(\"N° Clusters con mejor Score: \",clusters[pd.Series(silhouette_avg).idxmax()],\"\\nScore: \",max(silhouette_avg))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.1.3 Modelado K-Means con N° Óptimo Clusters 2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_clusters = clusters[pd.Series(silhouette_avg).idxmax()]\n",
    "kmeans = KMeans(n_clusters=n_clusters)\n",
    "label = kmeans.fit_predict(df_samples_PCA_2D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "axes_PCA = {'x': 'pca0', 'y': 'pca1'}\n",
    "title = f'K-means(K={n_clusters}) over 2D PCA transformed data'\n",
    "plot = Plot2D_WithLabels(df_samples_PCA_2D, kmeans.labels_, title, axes_PCA, centroids_2D=kmeans.cluster_centers_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.2 K-Means PCA 3D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.2.1 Búsqueda N° óptimo Clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca3D = PCA(n_components=3)\n",
    "df_3D = pca3D.fit_transform(embeddings_beto)\n",
    "df_samples_PCA_3D = pd.DataFrame(data=df_3D, columns=[\"pca0\", \"pca1\", \"pca2\"])\n",
    "\n",
    "clusters = [5,6,7,8,9,10,15]\n",
    "silhouette_avg = []\n",
    "\n",
    "for i in clusters:\n",
    "    #Initialize the class object\n",
    "    kmeans = KMeans(n_clusters=i)\n",
    "     \n",
    "    #predict the labels of clusters.\n",
    "    label = kmeans.fit_predict(df_samples_PCA_3D)\n",
    "     \n",
    "    #Getting unique labels\n",
    "    u_labels = np.unique(label)\n",
    "     \n",
    "    #plotting the results:\n",
    "    for i in u_labels:\n",
    "        plt.scatter(df_3D[label == i , 0] , df_3D[label == i , 1] , label = i)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    score = silhouette_score(df_samples_PCA_3D, kmeans.labels_)\n",
    "    silhouette_avg.append(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.2.2 Modelacion N° clusters - Score Silhouette"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(clusters,silhouette_avg)\n",
    "plt.xlabel(\"Values of K\") \n",
    "plt.ylabel(\"Silhouette score\") \n",
    "plt.title(\"Silhouette analysis For Optimal k\")\n",
    "plt.show()\n",
    "print(\"N° Clusters con mejor Score: \",clusters[pd.Series(silhouette_avg).idxmax()],\"\\nScore: \",max(silhouette_avg))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.2.3 Modelado K-Means con N° Óptimo Clusters 3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_clusters = clusters[pd.Series(silhouette_avg).idxmax()]\n",
    "kmeans = KMeans(n_clusters=n_clusters)\n",
    "label = kmeans.fit_predict(df_samples_PCA_3D)\n",
    "axes_PCA_3D = {'x': 'pca0', 'y': 'pca1', 'z': 'pca2'}\n",
    "\n",
    "title = f'K-means(K={n_clusters}) over 3D PCA transformed data'\n",
    "plot = Plot3D_WithLabels (df_samples_PCA_3D, kmeans.labels_, title, axes_PCA_3D, centroids_3D=kmeans.cluster_centers_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.3 DBScan t-SNE 2D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.3.1 Reducción de Dimensión"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne2D = TSNE(n_components=2)\n",
    "samples_TSNE_2D = tsne2D.fit_transform(embeddings_beto)\n",
    "df_samples_TSNE_2D = pd.DataFrame(data=samples_TSNE_2D, columns=[\"tsne0\", \"tsne1\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.3.2 Modelación Nearest Neighbor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PlotDistancesToKnearestNeighbor(samples_TSNE_2D, 50) # notese que el parametro de entrada NO es samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.3.3 Epsilons Score Silhouette "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_samples = 50\n",
    "epsilon_values = [4, 4.05, 4.1, 4.2, 4.3, 4.35, 4.4, 4.5, 4.6, 4.7, 4.9, 5, 5.2, 5.4, 5.6,6,6.2]\n",
    "aux_labels = {}\n",
    "for epsilon in epsilon_values:\n",
    "  aux_labels[epsilon] = ApplyDBScanToData (samples_TSNE_2D, epsilon, min_samples=min_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon = 4\n",
    "labels_TSNE_2D = aux_labels[epsilon]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.3.4 Visualización DBScan t-SNE 2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title = f'DBScan(eps={epsilon}, MinPts={min_samples}) over 2D t-SNE transformed data'\n",
    "Plot2D_WithLabels (samples_TSNE_2D, labels_TSNE_2D, title, axes_TSNE_2D)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.4 DBScan t-SNE 3D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.4.1 Reducción de Dimensión"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne3D = TSNE(n_components=3)\n",
    "samples_TSNE_3D = tsne3D.fit_transform(embeddings_beto)\n",
    "df_samples_TSNE_3D = pd.DataFrame(data=samples_TSNE_3D, columns=[\"tsne0\", \"tsne1\", \"tsne2\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.4.2 Modelación Nearest Neighbor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PlotDistancesToKnearestNeighbor(samples_TSNE_3D, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.4.3 DBScan t-SNE 3D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.3.4.3.1 Epsilons Score Silhouette"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_samples = 100\n",
    "epsilon_values = [5.1,5.2,5.25,5.3, 5.35, 5.4, 5.5, 5.6]\n",
    "aux_labels = {}\n",
    "for epsilon in epsilon_values:\n",
    "  aux_labels[epsilon] = ApplyDBScanToData (samples_TSNE_3D, epsilon, min_samples=min_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon = 5.4\n",
    "labels_TSNE_3D = aux_labels[epsilon]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.3.4.3.2 Visualización DBScan t-SNE 3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title = f'DBScan(eps={epsilon}, MinPts={min_samples}) over 3D t-SNE transformed data'\n",
    "axes_TSNE_3D = {'x': 'tsne0', 'y': 'tsne1' , 'z': 'tsne2'}\n",
    "Plot3D_WithLabels (samples_TSNE_3D, labels_TSNE_3D, title, axes_TSNE_3D)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.5 DBScan - PCA 2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 40\n",
    "PlotDistancesToKnearestNeighbor(df_samples_PCA_2D, n_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsis = [0.3,0.31,0.32,0.33,0.34,0.35,0.37,0.39,0.4,0.42,0.44,0.46,0.47,0.48,0.49,0.5]\n",
    "aux_labels = {}\n",
    "for epsilon in epsis:\n",
    "  aux_labels[epsilon] = ApplyDBScanToData (df_samples_PCA_2D, epsilon, min_samples=min_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon = 0.48\n",
    "labels_PCA_2D = aux_labels[epsilon]\n",
    "title = f'K-means(K={n_clusters}) over 2D PCA transformed data'\n",
    "Plot2D_WithLabels (df_samples_PCA_2D, labels_PCA_2D, title, axes_PCA_2D, palette=\"Paired\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.6 DBScan - PCA 3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 20\n",
    "PlotDistancesToKnearestNeighbor(df_samples_PCA_3D, n_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsis = [0.2,0.28,0.29,0.3,0.31,0.32,0.33,0.34,0.35,0.37,0.39,0.4,0.41,0.42,0.43,0.44,0.45,0.46,0.47,0.48]\n",
    "data_epsis = []\n",
    "min_samples = n_samples\n",
    "\n",
    "#epsis = [0.2,0.28,0.29,0.3,0.31,0.32,0.33,0.34,0.35,0.37,0.39,0.4]\n",
    "aux_labels = {}\n",
    "for epsilon in epsis:\n",
    "  aux_labels[epsilon] = ApplyDBScanToData (df_samples_PCA_3D, epsilon, min_samples=min_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon = 0.42\n",
    "labels_PCA_3D = aux_labels[epsilon]\n",
    "title = f'DBScan(K={n_clusters}, eps={epsilon}) over 3D PCA transformed data'\n",
    "Plot3D_WithLabels (df_samples_PCA_3D, labels_PCA_3D, title, axes_PCA_3D)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.7 K-Means t-SNE 3D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.7.1 Búsqueda N° óptimo Clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne3D = TSNE(n_components=3)\n",
    "df_3D = tsne3D.fit_transform(embeddings_beto)\n",
    "df_samples_tsne_3D = pd.DataFrame(data=df_3D, columns=[\"tsne0\", \"tsne1\", \"tsne2\"])\n",
    "\n",
    "clusters = [5,6,7,8,9,10,15]\n",
    "silhouette_avg = []\n",
    "\n",
    "for i in clusters:\n",
    "    #Initialize the class object\n",
    "    kmeans = KMeans(n_clusters=i)\n",
    "     \n",
    "    #predict the labels of clusters.\n",
    "    label = kmeans.fit_predict(df_samples_tsne_3D)\n",
    "     \n",
    "    #Getting unique labels\n",
    "    u_labels = np.unique(label)\n",
    "     \n",
    "    #plotting the results:\n",
    "    for i in u_labels:\n",
    "        plt.scatter(df_3D[label == i , 0] , df_3D[label == i , 1] , label = i)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    score = silhouette_score(df_samples_tsne_3D, kmeans.labels_)\n",
    "    silhouette_avg.append(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.7.2 Modelacion N° clusters - Score Silhouette"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(clusters,silhouette_avg)\n",
    "plt.xlabel(\"Values of K\") \n",
    "plt.ylabel(\"Silhouette score\") \n",
    "plt.title(\"Silhouette analysis For Optimal k\")\n",
    "plt.show()\n",
    "print(\"N° Clusters con mejor Score: \",clusters[pd.Series(silhouette_avg).idxmax()],\"\\nScore: \",max(silhouette_avg))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.2.3 Modelado K-Means con N° Óptimo Clusters 3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_clusters = clusters[pd.Series(silhouette_avg).idxmax()]\n",
    "kmeans = KMeans(n_clusters=n_clusters)\n",
    "label = kmeans.fit_predict(df_samples_tsne_3D)\n",
    "axes_tsne_3D = {'x': 'tsne0', 'y': 'tsne1', 'z': 'tsne2'}\n",
    "\n",
    "title = f'K-means(K={n_clusters}) over 3D t-SNE transformed data'\n",
    "plot = Plot3D_WithLabels (df_samples_tsne_3D, kmeans.labels_, title, axes_tsne_3D, centroids_3D=kmeans.cluster_centers_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.8 K-Means t-SNE 2D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.8.1 Búsqueda N° óptimo CLusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne2D = TSNE(n_components=2)\n",
    "df_2D = tsne2D.fit_transform(embeddings_beto)\n",
    "df_samples_TSNE_2D = pd.DataFrame(data=samples_TSNE_2D, columns=[\"tsne0\", \"tsne1\"])\n",
    "\n",
    "clusters = [5,6,7,8,9,10,15]\n",
    "silhouette_avg = []\n",
    "\n",
    "for i in clusters:\n",
    "    #Initialize the class object\n",
    "    kmeans = KMeans(n_clusters=i)\n",
    "     \n",
    "    #predict the labels of clusters.\n",
    "    label = kmeans.fit_predict(df_samples_TSNE_2D)\n",
    "     \n",
    "    #Getting unique labels\n",
    "    u_labels = np.unique(label)\n",
    "     \n",
    "    #plotting the results:\n",
    "    for i in u_labels:\n",
    "        plt.scatter(df_2D[label == i , 0] , df_2D[label == i , 1] , label = i)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    score = silhouette_score(df_samples_TSNE_2D, kmeans.labels_)\n",
    "    silhouette_avg.append(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.8.2 Modelacion N° clusters - Score Silhouette"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(clusters,silhouette_avg)\n",
    "plt.xlabel(\"Values of K\") \n",
    "plt.ylabel(\"Silhouette score\") \n",
    "plt.title(\"Silhouette analysis For Optimal k\")\n",
    "plt.show()\n",
    "print(\"N° Clusters con mejor Score: \",clusters[pd.Series(silhouette_avg).idxmax()],\"\\nScore: \",max(silhouette_avg))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.8.3 Modelado K-Means con N° Óptimo Clusters 2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_clusters = clusters[pd.Series(silhouette_avg).idxmax()]\n",
    "kmeans = KMeans(n_clusters=n_clusters)\n",
    "label = kmeans.fit_predict(df_samples_TSNE_2D)\n",
    "\n",
    "axes_PCA = {'x': 'tsne0', 'y': 'tsne1'}\n",
    "title = f'K-means(K={n_clusters}) over 2D tSNE transformed data'\n",
    "plot = Plot2D_WithLabels(df_samples_TSNE_2D, kmeans.labels_, title, axes_PCA, centroids_2D=kmeans.cluster_centers_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Embedding MARIA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Carga de Datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_maria = np.load(\"EmoEvalEs-embeddings-MARIA.npy\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Sin reducción de dimensionalidad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.1 Nearest Neighbor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PlotDistancesToKnearestNeighbor(embeddings_maria, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.2 DBScan sin Reducción de Dimensionalidad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_samples = 10\n",
    "epsilon_values = [0.8,1,1.2,1.3,1.4,1.5,1.6,1.7,1.8]\n",
    "aux_labels = {}\n",
    "for epsilon in epsilon_values:\n",
    "  aux_labels[epsilon] = ApplyDBScanToData(embeddings_maria, epsilon, min_samples=min_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon = 1.8\n",
    "labels = aux_labels[epsilon]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.3 K-Means sin PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(2,20):\n",
    "    km = KMeans(n_clusters=i, init='random', max_iter=200, random_state=0).fit(embeddings_maria)\n",
    "    sample_km = km.labels_\n",
    "    score = metrics.silhouette_score(embeddings_maria, sample_km)\n",
    "    print(\"cantidad de cluster:\", i)\n",
    "    print(\"Silhouette Coefficient: %0.3f\" % score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Con reducción de dimensionalidad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.1 K-Means PCA 2D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3.1.1 Búsqueda N° óptimo CLusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Raw Data\n",
    "pca2D = PCA(2)\n",
    " \n",
    "#Transform the data\n",
    "df_2D = pca2D.fit_transform(embeddings_maria)\n",
    "\n",
    "df_samples_PCA_2D = pd.DataFrame(data=df_2D, columns=[\"pca0\", \"pca1\"])\n",
    "\n",
    "clusters = [5,6,7,8,9,10,15]\n",
    "silhouette_avg = []\n",
    "\n",
    "for i in clusters:\n",
    "    #Initialize the class object\n",
    "    kmeans = KMeans(n_clusters=i)\n",
    "     \n",
    "    #predict the labels of clusters.\n",
    "    label = kmeans.fit_predict(df_samples_PCA_2D)\n",
    "     \n",
    "    #Getting unique labels\n",
    "    u_labels = np.unique(label)\n",
    "     \n",
    "    #plotting the results:\n",
    "    for i in u_labels:\n",
    "        plt.scatter(df_2D[label == i , 0] , df_2D[label == i , 1] , label = i)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    score = silhouette_score(df_samples_PCA_2D, kmeans.labels_)\n",
    "    silhouette_avg.append(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3.1.2 Modelacion N° clusters - Score Silhouette"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(clusters,silhouette_avg)\n",
    "plt.xlabel(\"Values of K\") \n",
    "plt.ylabel(\"Silhouette score\") \n",
    "plt.title(\"Silhouette analysis For Optimal k\")\n",
    "plt.show()\n",
    "print(\"N° Clusters con mejor Score: \",clusters[pd.Series(silhouette_avg).idxmax()],\"\\nScore: \",max(silhouette_avg))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3.1.3 Modelado K-Means con N° Óptimo Clusters 2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_clusters = clusters[pd.Series(silhouette_avg).idxmax()]\n",
    "kmeans = KMeans(n_clusters=n_clusters)\n",
    "label = kmeans.fit_predict(df_samples_PCA_2D)\n",
    "\n",
    "axes_PCA = {'x': 'pca0', 'y': 'pca1'}\n",
    "title = f'K-means(K={n_clusters}) over 2D PCA transformed data'\n",
    "plot = Plot2D_WithLabels(df_samples_PCA_2D, kmeans.labels_, title, axes_PCA, centroids_2D=kmeans.cluster_centers_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.2 K-Means PCA 3D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3.2.1 Búsqueda N° óptimo Clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca3D = PCA(n_components=3)\n",
    "df_3D = pca3D.fit_transform(embeddings_maria)\n",
    "df_samples_PCA_3D = pd.DataFrame(data=df_3D, columns=[\"pca0\", \"pca1\", \"pca2\"])\n",
    "\n",
    "clusters = [5,6,7,8,9,10,15]\n",
    "silhouette_avg = []\n",
    "\n",
    "for i in clusters:\n",
    "    #Initialize the class object\n",
    "    kmeans = KMeans(n_clusters=i)\n",
    "     \n",
    "    #predict the labels of clusters.\n",
    "    label = kmeans.fit_predict(df_samples_PCA_3D)\n",
    "     \n",
    "    #Getting unique labels\n",
    "    u_labels = np.unique(label)\n",
    "     \n",
    "    #plotting the results:\n",
    "    for i in u_labels:\n",
    "        plt.scatter(df_3D[label == i , 0] , df_3D[label == i , 1] , label = i)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    score = silhouette_score(df_samples_PCA_3D, kmeans.labels_)\n",
    "    silhouette_avg.append(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3.2.2 Modelacion N° clusters - Score Silhouette"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(clusters,silhouette_avg)\n",
    "plt.xlabel(\"Values of K\") \n",
    "plt.ylabel(\"Silhouette score\") \n",
    "plt.title(\"Silhouette analysis For Optimal k\")\n",
    "plt.show()\n",
    "print(\"N° Clusters con mejor Score: \",clusters[pd.Series(silhouette_avg).idxmax()],\"\\nScore: \",max(silhouette_avg))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3.2.3 Modelado K-Means con N° Óptimo Clusters 3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_clusters = clusters[pd.Series(silhouette_avg).idxmax()]\n",
    "kmeans = KMeans(n_clusters=n_clusters)\n",
    "label = kmeans.fit_predict(df_samples_PCA_3D)\n",
    "axes_PCA_3D = {'x': 'pca0', 'y': 'pca1', 'z': 'pca2'}\n",
    "\n",
    "title = f'K-means(K={n_clusters}) over 3D PCA transformed data'\n",
    "plot = Plot3D_WithLabels (df_samples_PCA_3D, kmeans.labels_, title, axes_PCA_3D, centroids_3D=kmeans.cluster_centers_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.3 t-SNE 2D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3.3.1 Reducción de Dimensión"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne2D = TSNE(n_components=2)\n",
    "samples_TSNE_2D = tsne2D.fit_transform(embeddings_maria)\n",
    "df_samples_TSNE_2D = pd.DataFrame(data=samples_TSNE_2D, columns=[\"tsne0\", \"tsne1\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3.3.2 Modelación Nearest Neighbor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PlotDistancesToKnearestNeighbor(samples_TSNE_2D, 50) # notese que el parametro de entrada NO es samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3.3.3 DBScan t-SNE 2D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.3.3.3.1 Epsilons Score Silhouette "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_samples = 50\n",
    "epsilon_values = [4.6, 4.7,4.8, 4.9, 4.95, 5, 5.05, 5.1, 5.2, 5.4, 5.6]\n",
    "aux_labels = {}\n",
    "for epsilon in epsilon_values:\n",
    "  aux_labels[epsilon] = ApplyDBScanToData (samples_TSNE_2D, epsilon, min_samples=min_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon = 4.95\n",
    "labels_TSNE_2D = aux_labels[epsilon]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.3.3.3.2 Visualización DBScan t-SNE 2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title = f'DBScan(eps={epsilon}, MinPts={min_samples}) over 2D t-SNE transformed data'\n",
    "Plot2D_WithLabels (samples_TSNE_2D, labels_TSNE_2D, title, axes_TSNE_2D)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.4 t-SNE 3D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3.4.1 Reducción de Dimensión"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne3D = TSNE(n_components=3)\n",
    "samples_TSNE_3D = tsne3D.fit_transform(embeddings_maria)\n",
    "df_samples_TSNE_3D = pd.DataFrame(data=samples_TSNE_3D, columns=[\"tsne0\", \"tsne1\", \"tsne2\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3.4.2 Modelación Nearest Neighbor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PlotDistancesToKnearestNeighbor(samples_TSNE_3D, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.4.3 DBScan t-SNE 3D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.3.4.3.1 Epsilons Score Silhouette"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_samples = 100\n",
    "epsilon_values = [4,4.1,4.2,4.3,4.4, 4.5, 4.6, 4.7, 4.8, 4.9, 5, 5.1,5.2]\n",
    "aux_labels = {}\n",
    "for epsilon in epsilon_values:\n",
    "  aux_labels[epsilon] = ApplyDBScanToData (samples_TSNE_3D, epsilon, min_samples=min_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon = 4.4\n",
    "labels_TSNE_3D = aux_labels[epsilon]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.3.4.3.2 Visualización DBScan t-SNE 3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title = f'DBScan(eps={epsilon}) over 3D t-SNE transformed data'\n",
    "axes_TSNE_3D = {'x': 'tsne0', 'y': 'tsne1' , 'z': 'tsne2'}\n",
    "Plot3D_WithLabels (samples_TSNE_3D, labels_TSNE_3D, title, axes_TSNE_3D)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.5 DBScan - PCA 2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 20\n",
    "PlotDistancesToKnearestNeighbor(df_samples_PCA_2D, n_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_samples = 20\n",
    "epsis = [0.025, 0.03,0.04, 0.05,0.06,0.07,0.08,0.09, 0.091, 0.092, 0.093, 0.097, 0.1, 0.11, 0.12, 0.13, 0.14, 0.15, 0.16, 0.17]\n",
    "aux_labels = {}\n",
    "for epsilon in epsis:\n",
    "  aux_labels[epsilon] = ApplyDBScanToData (df_samples_PCA_2D, epsilon, min_samples=min_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon = 0.13\n",
    "labels_PCA_2D = aux_labels[epsilon]\n",
    "title = f'DBScan(K={n_clusters}) over 2D PCA transformed data'\n",
    "Plot2D_WithLabels (df_samples_PCA_2D, labels_PCA_2D, title, axes_PCA_2D)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.6 DBScan - PCA 3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 20\n",
    "PlotDistancesToKnearestNeighbor(df_samples_PCA_3D, n_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsis = [0.07,0.09,0.0945,0.1,0.15,0.16,0.17,0.2]\n",
    "data_epsis = []\n",
    "min_samples = n_samples\n",
    "\n",
    "#epsis = [0.2,0.28,0.29,0.3,0.31,0.32,0.33,0.34,0.35,0.37,0.39,0.4]\n",
    "aux_labels = {}\n",
    "for epsilon in epsis:\n",
    "  aux_labels[epsilon] = ApplyDBScanToData (df_samples_PCA_3D, epsilon, min_samples=min_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon = 0.2\n",
    "labels_PCA_3D = aux_labels[epsilon]\n",
    "title = f'K-means(K={n_clusters}) over 3D PCA transformed data'\n",
    "Plot3D_WithLabels (df_samples_PCA_3D, labels_PCA_3D, title, axes_PCA_3D)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.7 K-Means t-SNE 3D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.7.1 Búsqueda N° óptimo Clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne3D = TSNE(n_components=3)\n",
    "df_3D = tsne3D.fit_transform(embeddings_maria)\n",
    "df_samples_tsne_3D = pd.DataFrame(data=df_3D, columns=[\"tsne0\", \"tsne1\", \"tsne2\"])\n",
    "\n",
    "clusters = [5,6,7,8,9,10,15]\n",
    "silhouette_avg = []\n",
    "\n",
    "for i in clusters:\n",
    "    #Initialize the class object\n",
    "    kmeans = KMeans(n_clusters=i)\n",
    "     \n",
    "    #predict the labels of clusters.\n",
    "    label = kmeans.fit_predict(df_samples_tsne_3D)\n",
    "     \n",
    "    #Getting unique labels\n",
    "    u_labels = np.unique(label)\n",
    "     \n",
    "    #plotting the results:\n",
    "    for i in u_labels:\n",
    "        plt.scatter(df_3D[label == i , 0] , df_3D[label == i , 1] , label = i)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    score = silhouette_score(df_samples_tsne_3D, kmeans.labels_)\n",
    "    silhouette_avg.append(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3.7.2 Modelacion N° clusters - Score Silhouette"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(clusters,silhouette_avg)\n",
    "plt.xlabel(\"Values of K\") \n",
    "plt.ylabel(\"Silhouette score\") \n",
    "plt.title(\"Silhouette analysis For Optimal k\")\n",
    "plt.show()\n",
    "print(\"N° Clusters con mejor Score: \",clusters[pd.Series(silhouette_avg).idxmax()],\"\\nScore: \",max(silhouette_avg))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.2.3 Modelado K-Means con N° Óptimo Clusters 3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_clusters = clusters[pd.Series(silhouette_avg).idxmax()]\n",
    "kmeans = KMeans(n_clusters=n_clusters)\n",
    "label = kmeans.fit_predict(df_samples_tsne_3D)\n",
    "axes_tsne_3D = {'x': 'tsne0', 'y': 'tsne1', 'z': 'tsne2'}\n",
    "\n",
    "title = f'K-means(K={n_clusters}) over 3D t-SNE transformed data'\n",
    "plot = Plot3D_WithLabels (df_samples_tsne_3D, kmeans.labels_, title, axes_tsne_3D, centroids_3D=kmeans.cluster_centers_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.8 K-Means t-SNE 2D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.8.1 Búsqueda N° óptimo CLusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne2D = TSNE(n_components=2)\n",
    "df_2D = tsne2D.fit_transform(embeddings_maria)\n",
    "df_samples_TSNE_2D = pd.DataFrame(data=samples_TSNE_2D, columns=[\"tsne0\", \"tsne1\"])\n",
    "\n",
    "clusters = [5,6,7,8,9,10,15]\n",
    "silhouette_avg = []\n",
    "\n",
    "for i in clusters:\n",
    "    #Initialize the class object\n",
    "    kmeans = KMeans(n_clusters=i)\n",
    "     \n",
    "    #predict the labels of clusters.\n",
    "    label = kmeans.fit_predict(df_samples_TSNE_2D)\n",
    "     \n",
    "    #Getting unique labels\n",
    "    u_labels = np.unique(label)\n",
    "     \n",
    "    #plotting the results:\n",
    "    for i in u_labels:\n",
    "        plt.scatter(df_2D[label == i , 0] , df_2D[label == i , 1] , label = i)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    score = silhouette_score(df_samples_TSNE_2D, kmeans.labels_)\n",
    "    silhouette_avg.append(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.8.2 Modelacion N° clusters - Score Silhouette"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(clusters,silhouette_avg)\n",
    "plt.xlabel(\"Values of K\") \n",
    "plt.ylabel(\"Silhouette score\") \n",
    "plt.title(\"Silhouette analysis For Optimal k\")\n",
    "plt.show()\n",
    "print(\"N° Clusters con mejor Score: \",clusters[pd.Series(silhouette_avg).idxmax()],\"\\nScore: \",max(silhouette_avg))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.8.3 Modelado K-Means con N° Óptimo Clusters 2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_clusters = clusters[pd.Series(silhouette_avg).idxmax()]\n",
    "kmeans = KMeans(n_clusters=n_clusters)\n",
    "label = kmeans.fit_predict(df_samples_TSNE_2D)\n",
    "\n",
    "axes_PCA = {'x': 'tsne0', 'y': 'tsne1'}\n",
    "title = f'K-means(K={n_clusters}) over 2D tSNE transformed data'\n",
    "plot = Plot2D_WithLabels(df_samples_TSNE_2D, kmeans.labels_, title, axes_PCA, centroids_2D=kmeans.cluster_centers_)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "APAU2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
